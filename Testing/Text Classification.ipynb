{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper Parameter Optimisation Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best Parameters: {'alpha': 0.0001, 'loss': 'hinge', 'max_iter': 100, 'penalty': 'l2', 'tol': None}\n",
      "\n",
      "Validation Accuracy: 0.919893899204244\n",
      "\n",
      "Validation Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.91      0.92        92\n",
      "           1       0.89      0.86      0.88        86\n",
      "           2       0.76      0.91      0.83        77\n",
      "           3       0.86      0.84      0.85        94\n",
      "           4       0.94      0.92      0.93        85\n",
      "           5       0.91      0.89      0.90       122\n",
      "           6       0.91      0.90      0.91       102\n",
      "           7       0.95      0.90      0.92        99\n",
      "           8       0.92      0.97      0.94       104\n",
      "           9       0.98      0.97      0.97        87\n",
      "          10       0.96      1.00      0.98        90\n",
      "          11       0.99      0.98      0.98        87\n",
      "          12       0.88      0.87      0.87       105\n",
      "          13       0.96      0.96      0.96       114\n",
      "          14       0.97      0.95      0.96       122\n",
      "          15       0.87      0.97      0.92       100\n",
      "          16       0.92      0.94      0.93        99\n",
      "          17       1.00      0.99      0.99        83\n",
      "          18       0.90      0.91      0.91        68\n",
      "          19       0.91      0.70      0.79        69\n",
      "\n",
      "    accuracy                           0.92      1885\n",
      "   macro avg       0.92      0.92      0.92      1885\n",
      "weighted avg       0.92      0.92      0.92      1885\n",
      "\n",
      "\n",
      "Test Accuracy: 0.9225464190981433\n",
      "\n",
      "Test Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.92      0.91       151\n",
      "           1       0.85      0.87      0.86       202\n",
      "           2       0.87      0.88      0.87       195\n",
      "           3       0.78      0.78      0.78       183\n",
      "           4       0.91      0.91      0.91       205\n",
      "           5       0.93      0.90      0.92       215\n",
      "           6       0.88      0.85      0.87       193\n",
      "           7       0.93      0.94      0.93       196\n",
      "           8       0.96      0.96      0.96       168\n",
      "           9       0.99      0.98      0.98       211\n",
      "          10       0.96      0.99      0.98       198\n",
      "          11       0.97      0.98      0.97       201\n",
      "          12       0.90      0.86      0.88       202\n",
      "          13       0.94      0.96      0.95       194\n",
      "          14       0.96      0.99      0.97       189\n",
      "          15       0.94      0.98      0.96       202\n",
      "          16       0.93      0.96      0.95       188\n",
      "          17       0.99      1.00      0.99       182\n",
      "          18       0.95      0.90      0.92       159\n",
      "          19       0.90      0.80      0.85       136\n",
      "\n",
      "    accuracy                           0.92      3770\n",
      "   macro avg       0.92      0.92      0.92      3770\n",
      "weighted avg       0.92      0.92      0.92      3770\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Step 1: Load the Dataset\n",
    "newsgroups = fetch_20newsgroups(subset='all')\n",
    "\n",
    "# Preprocess the Text (Add any preprocessing required)\n",
    "\n",
    "# Step 2: Feature Extraction (TF-IDF Vectorization)\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_df=0.5)\n",
    "X = vectorizer.fit_transform(newsgroups.data)\n",
    "y = newsgroups.target\n",
    "\n",
    "# Step 3: Split the Data (Training + Validation, and Test Set)\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 4: Split Training + Validation into Training and Validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.125, random_state=42)\n",
    "\n",
    "# Step 5: Set up SGDClassifier and GridSearchCV for Hyperparameter Tuning\n",
    "# Define parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'loss': ['hinge', 'log_loss'],               # hinge is SVM, log_loss is logistic regression\n",
    "    'penalty': ['l2', 'l1'],#, 'elasticnet'],  # Regularization types\n",
    "    'alpha': [1e-4],#, 1e-3, 1e-2],           # Regularization strength\n",
    "    'max_iter': [100],#, 200, 300],         # Number of epochs\n",
    "    'tol': [None]#[1e-3, 1e-4, None]               # Stopping criterion\n",
    "}\n",
    "\n",
    "# Initialize SGDClassifier\n",
    "clf = SGDClassifier(random_state=42)\n",
    "\n",
    "# Step 6: Perform Grid Search with Cross-Validation to Find the Best Hyperparameters\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=3, n_jobs=-1, verbose=2)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Step 7: Retrieve Best Hyperparameters\n",
    "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "\n",
    "# Step 8: Evaluate the Best Model on Validation Set\n",
    "best_model = grid_search.best_estimator_\n",
    "y_val_pred = best_model.predict(X_val)\n",
    "print(\"\\nValidation Accuracy:\", accuracy_score(y_val, y_val_pred))\n",
    "print(\"\\nValidation Classification Report:\\n\", classification_report(y_val, y_val_pred))\n",
    "\n",
    "# Step 9: Train the Best Model on Full Training Data (Training + Validation)\n",
    "best_model.fit(X_train_val, y_train_val)\n",
    "\n",
    "# Step 10: Evaluate on the Test Set\n",
    "y_test_pred = best_model.predict(X_test)\n",
    "print(\"\\nTest Accuracy:\", accuracy_score(y_test, y_test_pred))\n",
    "print(\"\\nTest Classification Report:\\n\", classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
      "\twith 203673 stored elements and shape (1885, 173446)>\n",
      "  Coords\tValues\n",
      "  (0, 117326)\t0.03520736385526365\n",
      "  (0, 127278)\t0.03414378326507175\n",
      "  (0, 86192)\t0.034998249575808695\n",
      "  (0, 142775)\t0.1003998181797237\n",
      "  (0, 158383)\t0.03673927189303268\n",
      "  (0, 153225)\t0.09653744214061885\n",
      "  (0, 51358)\t0.10881440012306128\n",
      "  (0, 36586)\t0.10553853795199583\n",
      "  (0, 80473)\t0.04963035075732454\n",
      "  (0, 126248)\t0.09488883531832912\n",
      "  (0, 63950)\t0.056199618813652735\n",
      "  (0, 164631)\t0.09846754125958262\n",
      "  (0, 100319)\t0.0916930094910735\n",
      "  (0, 103015)\t0.08554100023639419\n",
      "  (0, 46845)\t0.11152760387975934\n",
      "  (0, 133449)\t0.05599426571422951\n",
      "  (0, 126091)\t0.07033178984481478\n",
      "  (0, 129553)\t0.10389854340858318\n",
      "  (0, 65132)\t0.08475881961670258\n",
      "  (0, 131486)\t0.11623587611176993\n",
      "  (0, 135415)\t0.08092547129365302\n",
      "  (0, 124529)\t0.12420409404274306\n",
      "  (0, 124523)\t0.09088308373927181\n",
      "  (0, 4077)\t0.06843927845756899\n",
      "  (0, 140581)\t0.11330232967450017\n",
      "  :\t:\n",
      "  (1884, 69190)\t0.07700209410611938\n",
      "  (1884, 158449)\t0.06846632675102444\n",
      "  (1884, 123655)\t0.13542805854524176\n",
      "  (1884, 55783)\t0.22205330738651388\n",
      "  (1884, 147181)\t0.07767007320550402\n",
      "  (1884, 78983)\t0.1742842295287921\n",
      "  (1884, 73390)\t0.08311555764687115\n",
      "  (1884, 150825)\t0.07726492090491689\n",
      "  (1884, 146718)\t0.10874466706958304\n",
      "  (1884, 44173)\t0.08839664862649174\n",
      "  (1884, 120747)\t0.11062853869152615\n",
      "  (1884, 41567)\t0.09943057246029345\n",
      "  (1884, 79576)\t0.09224724625630558\n",
      "  (1884, 158523)\t0.10215760424770766\n",
      "  (1884, 64342)\t0.09591481701217598\n",
      "  (1884, 140625)\t0.09654167108639068\n",
      "  (1884, 62559)\t0.10711278324340869\n",
      "  (1884, 142811)\t0.10874466706958304\n",
      "  (1884, 135397)\t0.11558371768722721\n",
      "  (1884, 106309)\t0.11062853869152615\n",
      "  (1884, 96800)\t0.11285668589981297\n",
      "  (1884, 43676)\t0.11285668589981297\n",
      "  (1884, 110880)\t0.11909947313534465\n",
      "  (1884, 105536)\t0.11909947313534465\n",
      "  (1884, 2433)\t0.1240546521310457\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.datasets import fetch_20newsgroups\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.linear_model import SGDClassifier  # SVM with Stochastic Gradient Descent\n",
    "# from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# newsgroups = fetch_20newsgroups(subset='all')\n",
    "\n",
    "# vectorizer = TfidfVectorizer(stop_words='english', max_df=0.5)\n",
    "# dataX = vectorizer.fit_transform(newsgroups.data)\n",
    "# dataY = newsgroups.target\n",
    "\n",
    "# train_ratio = 0.75\n",
    "# validation_ratio = 0.15\n",
    "# test_ratio = 0.10\n",
    "\n",
    "# # train is now 75% of the entire data set\n",
    "# x_train, x_test, y_train, y_test = train_test_split(dataX, dataY, test_size=1 - train_ratio)\n",
    "\n",
    "# # test is now 10% of the initial data set\n",
    "# # validation is now 15% of the initial data set\n",
    "# x_val, x_test, y_val, y_test = train_test_split(x_test, y_test, test_size=test_ratio/(test_ratio + validation_ratio)) \n",
    "\n",
    "# # print(x_test, x_val, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8819628647214854\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.87      0.84       151\n",
      "           1       0.88      0.81      0.84       202\n",
      "           2       0.78      0.88      0.82       195\n",
      "           3       0.73      0.73      0.73       183\n",
      "           4       0.90      0.87      0.89       205\n",
      "           5       0.91      0.86      0.89       215\n",
      "           6       0.84      0.81      0.83       193\n",
      "           7       0.89      0.95      0.92       196\n",
      "           8       0.90      0.94      0.92       168\n",
      "           9       0.97      0.93      0.95       211\n",
      "          10       0.87      0.99      0.93       198\n",
      "          11       0.93      0.98      0.95       201\n",
      "          12       0.95      0.74      0.83       202\n",
      "          13       0.95      0.95      0.95       194\n",
      "          14       0.87      0.99      0.92       189\n",
      "          15       0.81      0.98      0.88       202\n",
      "          16       0.89      0.95      0.92       188\n",
      "          17       0.95      0.99      0.97       182\n",
      "          18       0.95      0.80      0.87       159\n",
      "          19       0.88      0.51      0.64       136\n",
      "\n",
      "    accuracy                           0.88      3770\n",
      "   macro avg       0.88      0.88      0.88      3770\n",
      "weighted avg       0.88      0.88      0.88      3770\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import SGDClassifier  # SVM with Stochastic Gradient Descent\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Step 1: Load the Dataset\n",
    "newsgroups = fetch_20newsgroups(subset='all')\n",
    "\n",
    "# Step 2: Preprocess the Text (Basic preprocessing)\n",
    "# For demonstration, we'll use raw data; further preprocessing can be applied if needed\n",
    "\n",
    "# Step 3: Feature Extraction\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_df=0.5)\n",
    "x = vectorizer.fit_transform(newsgroups.data)\n",
    "y = newsgroups.target\n",
    "\n",
    "# Step 4: Split Data into Training and Testing Sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 5: Train a Classification Model\n",
    "clf = SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3,\n",
    "                    random_state=42, max_iter=5, tol=None)\n",
    "clf.fit(x_train, y_train)\n",
    "\n",
    "# Step 6: Evaluate the Model\n",
    "y_pred = clf.predict(x_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The new text was classified as: sci.med\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Classify New Text\n",
    "def classify_new_text(text, vectorizer, clf):\n",
    "    # Preprocess and vectorize the input text using the same TfidfVectorizer\n",
    "    text_vectorized = vectorizer.transform([text])\n",
    "    \n",
    "    # Predict the category of the input text\n",
    "    predicted_category = clf.predict(text_vectorized)\n",
    "    \n",
    "    # Get the actual category name (since target names are stored in newsgroups.target_names)\n",
    "    category_name = newsgroups.target_names[predicted_category[0]]\n",
    "    \n",
    "    return category_name\n",
    "\n",
    "# Example of classifying new text\n",
    "new_text = \"Steve went and shot up the school because of Trump\"#\"Apple just released a new iPhone with improved camera technology.\"\n",
    "predicted_category = classify_new_text(new_text, vectorizer, clf)\n",
    "print(f\"The new text was classified as: {predicted_category}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternate ChatGPT implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Install the required libraries\n",
    "# !pip install scikit-learn numpy pandas nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.datasets import fetch_20newsgroups\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# # Load the 20 newsgroups dataset\n",
    "# categories = ['alt.atheism', 'comp.graphics', 'sci.med', 'rec.sport.baseball']\n",
    "# newsgroups_data = fetch_20newsgroups(subset='all', categories=categories, shuffle=True, random_state=42)\n",
    "\n",
    "# # Split the data into train and test sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(newsgroups_data.data, newsgroups_data.target, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Initialize a TF-IDF Vectorizer to convert text to numerical vectors\n",
    "# tfidf = TfidfVectorizer(stop_words='english', max_df=0.7)\n",
    "# X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "# X_test_tfidf = tfidf.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.naive_bayes import MultinomialNB\n",
    "# from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# # Train a Naive Bayes classifier\n",
    "# nb_classifier = MultinomialNB()\n",
    "# nb_classifier.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# # Predict on the test set\n",
    "# y_pred = nb_classifier.predict(X_test_tfidf)\n",
    "\n",
    "# # Evaluate the classifier\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# print(f\"Naive Bayes Accuracy: {accuracy * 100:.2f}%\")\n",
    "# print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred, target_names=newsgroups_data.target_names))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
